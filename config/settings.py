import os
from dotenv import load_dotenv
from llama_index.core import Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
import streamlit as st
from pathlib import Path

# LLMs y embeddings
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

load_dotenv()

class Config:
    # üî• FORZAR MODO LOCAL (ignorar .env)
    MODE = "local"  # ‚úÖ HARDCODEADO - NO usar os.getenv()

    # Local model
    PDF_DIR = os.getenv("PDF_DIR", "./data/raw/pdfs")  

    # üî• NUEVO MODELO LLAMA 3.2 3B - MUCHO M√ÅS R√ÅPIDO
    MISTRAL_MODEL_PATH = Path(r"./models/llama32-3b/Llama-3.2-3B-Instruct-Q4_K_M.gguf")

    INDEX_DIR = Path(os.getenv("INDEX_DIR", "./data/processed/index")) 
    
    # Vector DB
    VECTOR_DB_TYPE = os.getenv("VECTOR_DB_TYPE", "chroma")  
    CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH", "./data/processed/chroma_db")
    COLLECTION_NAME = os.getenv("COLLECTION_NAME", "safety_sheets")
    
    # üî• CHUNKING - MANTENER CONFIGURACI√ìN ORIGINAL
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 1024))  # ‚úÖ ORIGINAL
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", 200))  # ‚úÖ ORIGINAL
    
    # üî• QUERY - MANTENER CONFIGURACI√ìN ORIGINAL
    TOP_K = int(os.getenv("TOP_K", 5))  # ‚úÖ ORIGINAL
    TEMPERATURE = float(os.getenv("TEMPERATURE", 0.1))

    # üî• PROMPTS CENTRALIZADOS - M√ÅS CONCISOS
    PROMPTS = {
        "system_prompt": """Eres un experto asistente de IA de Syngenta especializado en agricultura. 
Responde siempre en espa√±ol con tono profesional pero accesible. 
Enf√≥cate en soluciones pr√°cticas para agricultores.""",

        "qa_template": """Eres un experto asistente de IA de Syngenta especializado en agricultura.

Contexto relevante:
{context_str}

Pregunta del usuario: {query_str}

Instrucciones:
- Responde en espa√±ol como experto en agricultura de Syngenta
- Usa SOLO informaci√≥n del contexto proporcionado
- Si no tienes informaci√≥n suficiente, dilo claramente
- Incluye referencias espec√≠ficas cuando sea posible
- Enf√≥cate en soluciones pr√°cticas para agricultores

Respuesta:"""
    }

    # üî• SYSTEM PROMPT OPTIMIZADO PARA LLAMA 3.2
    LLM_SYSTEM_PROMPT = """Eres un asistente especializado en seguridad de productos Syngenta. 
SIEMPRE responde en ESPA√ëOL. 
Proporciona respuestas concisas y espec√≠ficas basadas √∫nicamente en la informaci√≥n de las fichas de seguridad proporcionadas.
Si no encuentras informaci√≥n espec√≠fica, di "No encuentro esa informaci√≥n en las fichas disponibles"."""

    # üî• CONFIGURACI√ìN QUERY ENGINE - MANTENER ORIGINAL
    QUERY_ENGINE_CONFIG = {
        "similarity_top_k": 3,  # ‚úÖ ORIGINAL
        "response_mode": "compact",
        "streaming": False
    }

# Instancia global
settings = Config()

def setup_llama_index():
    """Configura LlamaIndex FORZANDO modo local con Llama 3.2 3B"""
    
    # üî• LOGS DETALLADOS
    print(f"üîß MODO CONFIGURADO: {settings.MODE}")
    print(f"ü§ñ MODELO PATH: {settings.MISTRAL_MODEL_PATH}")
    print(f"üìÅ ARCHIVO EXISTE: {settings.MISTRAL_MODEL_PATH.exists()}")
    print(f"üìä TAMA√ëO ARCHIVO: {settings.MISTRAL_MODEL_PATH.stat().st_size / (1024**3):.2f} GB" if settings.MISTRAL_MODEL_PATH.exists() else "‚ùå NO EXISTE")

    # üî• VERIFICAR QUE EL ARCHIVO EXISTE
    if not settings.MISTRAL_MODEL_PATH.exists():
        raise FileNotFoundError(f"‚ùå MODELO NO ENCONTRADO: {settings.MISTRAL_MODEL_PATH}")
    
    print("üöÄ Inicializando LlamaCPP con Llama 3.2 3B LOCAL...")
    
    # üî• LLM LOCAL - PAR√ÅMETROS CORREGIDOS
    llm = LlamaCPP(
        model_path=str(settings.MISTRAL_MODEL_PATH),
        temperature=settings.TEMPERATURE,
        max_new_tokens=512,  # ‚úÖ MANTENER ORIGINAL
        context_window=4096,  # ‚úÖ MANTENER ORIGINAL
        verbose=True,
        system_prompt=settings.PROMPTS["system_prompt"],  # üî• USAR PROMPT NUEVO Y CONCISO
        # üî• TODOS LOS PAR√ÅMETROS ESPEC√çFICOS VAN EN model_kwargs
        model_kwargs={
            #"n_ctx": 1024,  # ‚úÖ MOVIDO A model_kwargs
            "n_batch": 256,  # ‚úÖ MOVIDO A model_kwargs
            "n_threads": None,  # ‚úÖ MOVIDO A model_kwargs (auto-detect)
            "use_mmap": True,  # ‚úÖ MOVIDO A model_kwargs
            "use_mlock": False,  # ‚úÖ MOVIDO A model_kwargs
            "n_gpu_layers": 0,  # Sin GPU para CPU puro
            "f16_kv": True,  # Float16 para KV cache
        }
    )
    
    print("‚úÖ LlamaCPP inicializado correctamente con Llama 3.2 3B")
    print("üöÄ Inicializando embeddings locales...")
    
    # üî• EMBEDDINGS LOCALES
    embed_model = HuggingFaceEmbedding(
        model_name="BAAI/bge-small-en-v1.5",
        cache_folder="./model/embeddings"
    )
    
    print("‚úÖ Embeddings locales inicializados")
    
    # Vector store
    chroma_client = chromadb.PersistentClient(path=settings.CHROMA_DB_PATH)
    chroma_collection = chroma_client.get_or_create_collection(settings.COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    
    # Settings globales - MANTENER CONFIGURACI√ìN ORIGINAL
    Settings.llm = llm
    Settings.embed_model = embed_model
    Settings.chunk_size = settings.CHUNK_SIZE  # ‚úÖ 1024 ORIGINAL
    Settings.chunk_overlap = settings.CHUNK_OVERLAP  # ‚úÖ 200 ORIGINAL
    
    print("üéØ CONFIGURACI√ìN COMPLETADA - LLAMA 3.2 3B CON CONFIGURACI√ìN ORIGINAL")
    
    return llm, embed_model, vector_store

def get_chroma_collection():
    """Obtiene la colecci√≥n de Chroma"""
    chroma_client = chromadb.PersistentClient(path=settings.CHROMA_DB_PATH)
    return chroma_client.get_or_create_collection(settings.COLLECTION_NAME)

# Test directo
if __name__ == "__main__":
    print("üî• PROBANDO CONFIGURACI√ìN LOCAL CON LLAMA 3.2 3B...")
    llm, embed_model, vector_store = setup_llama_index()
    print(f"‚úÖ LlamaIndex configurado en modo {settings.MODE}")
    print(f"ü§ñ LLM tipo: {type(llm)}")
    print(f"üìä Embed tipo: {type(embed_model)}")
    
    # üî• TEST DEL SYSTEM PROMPT
    print("\nüß™ PROBANDO SYSTEM PROMPT...")
    try:
        response = llm.complete("¬øQu√© es Syngenta?")
        print(f"‚úÖ RESPUESTA: {response}")
    except Exception as e:
        print(f"‚ùå ERROR: {e}")